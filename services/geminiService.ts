import { GoogleGenAI, HarmCategory, HarmBlockThreshold } from "@google/genai";
import { GeminiModel } from "../types";

/**
 * Generates or edits an image based on an input image and a text prompt.
 * Uses Gemini 2.5 Flash Image.
 * 
 * @param imageBase64 - The input image in base64 format (stripped of data:image/xyz;base64, prefix if possible, but the API handles cleaned base64)
 * @param prompt - The text instruction for the model
 * @param mimeType - The mime type of the input image
 * @returns Promise resolving to the base64 string of the generated image
 */
export const generateOrEditImage = async (
  imageBase64: string,
  prompt: string,
  mimeType: string = 'image/jpeg'
): Promise<string> => {
  try {
    // Initialize the Gemini client
    // process.env.API_KEY is injected by the environment
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

    // Ensure clean base64 string
    const cleanBase64 = imageBase64.replace(/^data:image\/\w+;base64,/, "");

    const response = await ai.models.generateContent({
      model: GeminiModel.FLASH_IMAGE,
      contents: {
        parts: [
          {
            inlineData: {
              data: cleanBase64,
              mimeType: mimeType,
            },
          },
          {
            text: prompt,
          },
        ],
      },
      config: {
        // Config for gemini-2.5-flash-image
        imageConfig: {
          aspectRatio: "3:4", // Matches UI vertical aspect ratio
        },
        // Relax safety settings slightly to prevent false positives on standard portraits
        safetySettings: [
          { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
          { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
          { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
          { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
        ],
      }
    });

    const candidate = response.candidates?.[0];

    // Check for safety finish reason if no content
    if (!candidate?.content) {
        // Handle "IMAGE_OTHER" or "OTHER" which often means the model refused the request due to policy 
        // (often triggered by "exact identity" requests on real people).
        const reason = candidate?.finishReason;
        if (reason === 'OTHER' || reason === 'IMAGE_OTHER') {
             throw new Error("The AI model was unable to process this image. It may contain content that triggers safety or quality filters. Please try a different photo or style.");
        }
        if (reason) {
             throw new Error(`Generation stopped. Reason: ${reason}`);
        }
        throw new Error("No content generated by the model.");
    }

    // Iterate through parts to find the image
    const parts = candidate.content.parts;
    
    if (!parts || parts.length === 0) {
       if (candidate?.finishReason) {
             throw new Error(`Generation stopped. Reason: ${candidate.finishReason}`);
        }
       throw new Error("No content generated");
    }

    for (const part of parts) {
      if (part.inlineData) {
        return part.inlineData.data;
      }
    }

    // If no image part found, maybe there's text explaining why (safety, etc)
    const textPart = parts.find(p => p.text);
    if (textPart) {
      throw new Error(`Generation returned text instead of image: "${textPart.text}"`);
    }

    throw new Error("No image data found in response");

  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};